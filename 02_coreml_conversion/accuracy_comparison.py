#!/usr/bin/env python3
"""
æ¨¡å‹ç²¾åº¦å¯¹æ¯”å·¥å…·
æ¯”è¾ƒ PyTorch åŸå§‹æ¨¡å‹å’Œ CoreML è½¬æ¢æ¨¡å‹çš„æ£€æµ‹ç²¾åº¦
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Tuple, Any
import numpy as np
from ultralytics import YOLO
import requests
from PIL import Image
from io import BytesIO
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

class AccuracyComparator:
    def __init__(self, pytorch_model_path: str, coreml_model_path: str):
        """
        åˆå§‹åŒ–ç²¾åº¦å¯¹æ¯”å™¨
        
        Args:
            pytorch_model_path: PyTorch æ¨¡å‹è·¯å¾„
            coreml_model_path: CoreML æ¨¡å‹è·¯å¾„
        """
        self.pytorch_model_path = Path(pytorch_model_path)
        self.coreml_model_path = Path(coreml_model_path)
        
        # åŠ è½½æ¨¡å‹
        self.pytorch_model = None
        self.coreml_model = None
        self.load_models()
        
        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.results = {
            'pytorch': [],
            'coreml': [],
            'comparison': {}
        }
    
    def load_models(self):
        """åŠ è½½ä¸¤ä¸ªæ¨¡å‹"""
        try:
            print("ğŸ”„ åŠ è½½ PyTorch æ¨¡å‹...")
            self.pytorch_model = YOLO(str(self.pytorch_model_path))
            print("âœ… PyTorch æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            print("ğŸ”„ åŠ è½½ CoreML æ¨¡å‹...")
            self.coreml_model = YOLO(str(self.coreml_model_path))
            print("âœ… CoreML æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    
    def prepare_test_dataset(self, dataset_name: str = "auto") -> List[str]:
        """å‡†å¤‡æµ‹è¯•æ•°æ®é›†"""
        print("ğŸ“¥ å‡†å¤‡æµ‹è¯•æ•°æ®é›†...")
        
        # å¯¼å…¥æ•°æ®é›†ç®¡ç†å™¨
        from dataset_manager import DatasetManager
        
        dataset_manager = DatasetManager()
        
        if dataset_name == "auto":
            # è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¯ç”¨æ•°æ®é›†
            print("ğŸ” è‡ªåŠ¨é€‰æ‹©æµ‹è¯•æ•°æ®é›†...")
            
            # ä¼˜å…ˆçº§é¡ºåºï¼šCOCOæ ·æœ¬ > è‡ªå®šä¹‰æ ·æœ¬ > YOLOæµ‹è¯•å›¾ç‰‡
            preferred_datasets = ["coco_val_sample", "custom_sample", "yolo_test_images"]
            
            for dataset in preferred_datasets:
                images = dataset_manager.get_dataset_images(dataset)
                if images:
                    print(f"âœ… ä½¿ç”¨ç°æœ‰æ•°æ®é›†: {dataset} ({len(images)} å¼ å›¾ç‰‡)")
                    return images
            
            # å¦‚æœæ²¡æœ‰ç°æœ‰æ•°æ®é›†ï¼Œä¸‹è½½é»˜è®¤æ•°æ®é›†
            print("ğŸ“¥ ä¸‹è½½é»˜è®¤æµ‹è¯•æ•°æ®é›†...")
            dataset_path = dataset_manager.download_dataset("coco_val_sample")
            if dataset_path:
                images = dataset_manager.get_dataset_images("coco_val_sample")
                print(f"âœ… ä½¿ç”¨æ–°ä¸‹è½½çš„æ•°æ®é›†: coco_val_sample ({len(images)} å¼ å›¾ç‰‡)")
                return images
            
            # å¤‡é€‰æ–¹æ¡ˆï¼šä¸‹è½½ YOLO æµ‹è¯•å›¾ç‰‡
            print("ğŸ“¥ ä¸‹è½½ YOLO æµ‹è¯•å›¾ç‰‡ä½œä¸ºå¤‡é€‰...")
            dataset_path = dataset_manager.download_dataset("yolo_test_images")
            if dataset_path:
                images = dataset_manager.get_dataset_images("yolo_test_images")
                return images
        
        else:
            # ä½¿ç”¨æŒ‡å®šçš„æ•°æ®é›†
            images = dataset_manager.get_dataset_images(dataset_name)
            if images:
                print(f"âœ… ä½¿ç”¨æŒ‡å®šæ•°æ®é›†: {dataset_name} ({len(images)} å¼ å›¾ç‰‡)")
                return images
            
            # å°è¯•ä¸‹è½½æŒ‡å®šæ•°æ®é›†
            print(f"ğŸ“¥ ä¸‹è½½æ•°æ®é›†: {dataset_name}")
            dataset_path = dataset_manager.download_dataset(dataset_name)
            if dataset_path:
                images = dataset_manager.get_dataset_images(dataset_name)
                return images
        
        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨æœ¬åœ°å›¾ç‰‡
        print("âš ï¸  ä½¿ç”¨æœ¬åœ°å¤‡é€‰å›¾ç‰‡...")
        fallback_images = []
        local_test_image = project_root / "shared_resources" / "test_images" / "bus.jpg"
        if local_test_image.exists():
            fallback_images.append(str(local_test_image))
        
        if not fallback_images:
            print("âŒ æ— æ³•è·å–æµ‹è¯•å›¾ç‰‡")
            return []
        
        return fallback_images
    
    def run_inference(self, model: YOLO, image_path: str, conf_threshold: float = 0.25) -> List[Dict]:
        """
        è¿è¡Œæ¨ç†å¹¶è¿”å›æ ‡å‡†åŒ–ç»“æœ
        
        Args:
            model: YOLO æ¨¡å‹
            image_path: å›¾ç‰‡è·¯å¾„
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        try:
            results = model.predict(image_path, conf=conf_threshold, verbose=False)
            
            detections = []
            if results and results[0].boxes is not None:
                for box in results[0].boxes:
                    detection = {
                        'class_id': int(box.cls.cpu().numpy()[0]),
                        'class_name': model.names[int(box.cls.cpu().numpy()[0])],
                        'confidence': float(box.conf.cpu().numpy()[0]),
                        'bbox': box.xyxy.cpu().numpy()[0].tolist(),  # [x1, y1, x2, y2]
                        'area': float((box.xyxy[0][2] - box.xyxy[0][0]) * (box.xyxy[0][3] - box.xyxy[0][1]))
                    }
                    detections.append(detection)
            
            return detections
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            return []
    
    def calculate_iou(self, box1: List[float], box2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„ IoU (Intersection over Union)
        
        Args:
            box1, box2: [x1, y1, x2, y2] æ ¼å¼çš„è¾¹ç•Œæ¡†
            
        Returns:
            IoU å€¼
        """
        # è®¡ç®—äº¤é›†åŒºåŸŸ
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        if x2 <= x1 or y2 <= y1:
            return 0.0
        
        intersection = (x2 - x1) * (y2 - y1)
        
        # è®¡ç®—å¹¶é›†åŒºåŸŸ
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def match_detections(self, pytorch_dets: List[Dict], coreml_dets: List[Dict], 
                        iou_threshold: float = 0.5) -> Tuple[List, List, List]:
        """
        åŒ¹é…ä¸¤ä¸ªæ¨¡å‹çš„æ£€æµ‹ç»“æœ
        
        Args:
            pytorch_dets: PyTorch æ¨¡å‹æ£€æµ‹ç»“æœ
            coreml_dets: CoreML æ¨¡å‹æ£€æµ‹ç»“æœ  
            iou_threshold: IoU åŒ¹é…é˜ˆå€¼
            
        Returns:
            (matched_pairs, unmatched_pytorch, unmatched_coreml)
        """
        matched_pairs = []
        unmatched_pytorch = list(range(len(pytorch_dets)))
        unmatched_coreml = list(range(len(coreml_dets)))
        
        # è®¡ç®—æ‰€æœ‰æ£€æµ‹æ¡†ä¹‹é—´çš„ IoU
        for i, pt_det in enumerate(pytorch_dets):
            best_iou = 0
            best_match = -1
            
            for j, cm_det in enumerate(coreml_dets):
                if j not in unmatched_coreml:
                    continue
                    
                # åªåŒ¹é…ç›¸åŒç±»åˆ«çš„æ£€æµ‹
                if pt_det['class_id'] != cm_det['class_id']:
                    continue
                
                iou = self.calculate_iou(pt_det['bbox'], cm_det['bbox'])
                if iou > best_iou and iou >= iou_threshold:
                    best_iou = iou
                    best_match = j
            
            if best_match != -1:
                matched_pairs.append({
                    'pytorch_idx': i,
                    'coreml_idx': best_match,
                    'iou': best_iou,
                    'pytorch_det': pt_det,
                    'coreml_det': coreml_dets[best_match],
                    'conf_diff': abs(pt_det['confidence'] - coreml_dets[best_match]['confidence'])
                })
                unmatched_pytorch.remove(i)
                unmatched_coreml.remove(best_match)
        
        return matched_pairs, unmatched_pytorch, unmatched_coreml
    
    def calculate_ap_for_class(self, all_detections: List[Dict], class_id: int, 
                              iou_threshold: float = 0.5) -> float:
        """
        è®¡ç®—ç‰¹å®šç±»åˆ«çš„ Average Precision (AP)
        
        Args:
            all_detections: æ‰€æœ‰æ£€æµ‹ç»“æœï¼ŒåŒ…å« pytorch å’Œ coreml çš„åŒ¹é…ä¿¡æ¯
            class_id: ç±»åˆ« ID
            iou_threshold: IoU é˜ˆå€¼
            
        Returns:
            è¯¥ç±»åˆ«çš„ AP å€¼
        """
        # æ”¶é›†è¯¥ç±»åˆ«çš„æ‰€æœ‰æ£€æµ‹
        class_detections = []
        
        for detection_data in all_detections:
            pytorch_dets = detection_data['pytorch_details']
            coreml_dets = detection_data['coreml_details'] 
            matched_pairs = detection_data['matched_details']
            
            # è·å–è¯¥ç±»åˆ«çš„ PyTorch çœŸå€¼æ£€æµ‹ï¼ˆä½œä¸º Ground Truthï¼‰
            gt_boxes = [det for det in pytorch_dets if det['class_id'] == class_id]
            # è·å–è¯¥ç±»åˆ«çš„ CoreML é¢„æµ‹æ£€æµ‹
            pred_boxes = [det for det in coreml_dets if det['class_id'] == class_id]
            
            if not gt_boxes and not pred_boxes:
                continue
            
            # è®¡ç®—è¯¥å›¾ç‰‡è¯¥ç±»åˆ«çš„æ£€æµ‹ç»“æœ
            for pred in pred_boxes:
                # æ‰¾åˆ°æœ€åŒ¹é…çš„çœŸå€¼æ¡†
                best_iou = 0
                is_true_positive = False
                
                for gt in gt_boxes:
                    iou = self.calculate_iou(pred['bbox'], gt['bbox'])
                    if iou > best_iou:
                        best_iou = iou
                        if iou >= iou_threshold:
                            is_true_positive = True
                
                class_detections.append({
                    'confidence': pred['confidence'],
                    'is_tp': is_true_positive,
                    'iou': best_iou
                })
        
        if not class_detections:
            return 0.0
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        class_detections.sort(key=lambda x: x['confidence'], reverse=True)
        
        # è®¡ç®— Precision-Recall æ›²çº¿
        tp_count = 0
        total_gt = sum(len([det for det in data['pytorch_details'] if det['class_id'] == class_id]) 
                      for data in all_detections)
        
        if total_gt == 0:
            return 0.0
        
        precisions = []
        recalls = []
        
        for i, det in enumerate(class_detections):
            if det['is_tp']:
                tp_count += 1
            
            precision = tp_count / (i + 1)
            recall = tp_count / total_gt
            
            precisions.append(precision)
            recalls.append(recall)
        
        # è®¡ç®— AP (ä½¿ç”¨æ¢¯å½¢è§„åˆ™ç§¯åˆ†)
        if len(recalls) < 2:
            return precisions[0] if precisions else 0.0
        
        # 11ç‚¹æ’å€¼æ–¹æ³•è®¡ç®— AP
        ap = 0.0
        for recall_threshold in np.arange(0, 1.1, 0.1):
            # æ‰¾åˆ° recall >= threshold çš„æœ€å¤§ precision
            max_precision = 0.0
            for p, r in zip(precisions, recalls):
                if r >= recall_threshold:
                    max_precision = max(max_precision, p)
            ap += max_precision
        
        return ap / 11.0
    
    def calculate_map(self, all_results: List[Dict], iou_threshold: float = 0.5) -> Dict:
        """
        è®¡ç®— mAP (mean Average Precision)
        
        Args:
            all_results: æ‰€æœ‰å›¾ç‰‡çš„æ£€æµ‹ç»“æœ
            iou_threshold: IoU é˜ˆå€¼
            
        Returns:
            åŒ…å« mAP å’Œå„ç±»åˆ« AP çš„å­—å…¸
        """
        print(f"ğŸ“Š è®¡ç®— mAP (IoU={iou_threshold})...")
        
        # è·å–æ‰€æœ‰å‡ºç°çš„ç±»åˆ«
        all_classes = set()
        for result in all_results:
            for det in result['pytorch_details']:
                all_classes.add(det['class_id'])
            for det in result['coreml_details']:
                all_classes.add(det['class_id'])
        
        if not all_classes:
            return {'mAP': 0.0, 'class_APs': {}}
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ AP
        class_aps = {}
        valid_aps = []
        
        for class_id in sorted(all_classes):
            ap = self.calculate_ap_for_class(all_results, class_id, iou_threshold)
            
            # è·å–ç±»åˆ«åç§°
            class_name = "unknown"
            for result in all_results:
                for det in result['pytorch_details'] + result['coreml_details']:
                    if det['class_id'] == class_id:
                        class_name = det['class_name']
                        break
                if class_name != "unknown":
                    break
            
            class_aps[class_name] = ap
            if ap > 0:  # åªè®¡ç®—æœ‰æ•ˆçš„ AP å€¼
                valid_aps.append(ap)
            
            print(f"   {class_name} (ID:{class_id}): AP = {ap:.3f}")
        
        # è®¡ç®— mAP
        map_score = np.mean(valid_aps) if valid_aps else 0.0
        
        return {
            'mAP': map_score,
            'class_APs': class_aps,
            'num_classes': len(all_classes),
            'valid_classes': len(valid_aps)
        }
    
    def analyze_single_image(self, image_path: str, conf_threshold: float = 0.25) -> Dict:
        """
        åˆ†æå•å¼ å›¾ç‰‡çš„ç²¾åº¦å¯¹æ¯”
        """
        print(f"ğŸ” åˆ†æå›¾ç‰‡: {Path(image_path).name}")
        
        # è¿è¡Œä¸¤ä¸ªæ¨¡å‹çš„æ¨ç†
        pytorch_dets = self.run_inference(self.pytorch_model, image_path, conf_threshold)
        coreml_dets = self.run_inference(self.coreml_model, image_path, conf_threshold)
        
        # åŒ¹é…æ£€æµ‹ç»“æœ
        matched_pairs, unmatched_pt, unmatched_cm = self.match_detections(pytorch_dets, coreml_dets)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        analysis = {
            'image_path': image_path,
            'pytorch_detections': len(pytorch_dets),
            'coreml_detections': len(coreml_dets),
            'matched_pairs': len(matched_pairs),
            'unmatched_pytorch': len(unmatched_pt),
            'unmatched_coreml': len(unmatched_cm),
            'precision': len(matched_pairs) / len(coreml_dets) if len(coreml_dets) > 0 else 0,
            'recall': len(matched_pairs) / len(pytorch_dets) if len(pytorch_dets) > 0 else 0,
            'matched_details': matched_pairs,
            'pytorch_details': pytorch_dets,
            'coreml_details': coreml_dets
        }
        
        # è®¡ç®— F1 åˆ†æ•°
        if analysis['precision'] + analysis['recall'] > 0:
            analysis['f1_score'] = 2 * (analysis['precision'] * analysis['recall']) / (analysis['precision'] + analysis['recall'])
        else:
            analysis['f1_score'] = 0
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦å·®å¼‚
        if matched_pairs:
            analysis['avg_confidence_diff'] = np.mean([pair['conf_diff'] for pair in matched_pairs])
            analysis['avg_iou'] = np.mean([pair['iou'] for pair in matched_pairs])
        else:
            analysis['avg_confidence_diff'] = 0
            analysis['avg_iou'] = 0
        
        print(f"   PyTorch: {len(pytorch_dets)} æ£€æµ‹, CoreML: {len(coreml_dets)} æ£€æµ‹")
        print(f"   åŒ¹é…: {len(matched_pairs)}, ç²¾ç¡®åº¦: {analysis['precision']:.3f}, å¬å›ç‡: {analysis['recall']:.3f}")
        
        return analysis
    
    def run_full_comparison(self, conf_thresholds: List[float] = [0.1, 0.25, 0.5], 
                           dataset_name: str = "auto") -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„ç²¾åº¦å¯¹æ¯”åˆ†æ
        
        Args:
            conf_thresholds: ç½®ä¿¡åº¦é˜ˆå€¼åˆ—è¡¨
            dataset_name: æµ‹è¯•æ•°æ®é›†åç§° ("auto", "coco_val_sample", "custom_sample", "yolo_test_images")
        """
        print("ğŸš€ å¼€å§‹æ¨¡å‹ç²¾åº¦å¯¹æ¯”åˆ†æ")
        print("=" * 60)
        
        # è·å–æµ‹è¯•æ•°æ®é›†
        test_images = self.prepare_test_dataset(dataset_name)
        if not test_images:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•å›¾ç‰‡")
            return {}
        
        print(f"ğŸ“Š å°†åœ¨ {len(test_images)} å¼ å›¾ç‰‡ä¸Šè¿›è¡Œç²¾åº¦æµ‹è¯•")
        
        # å¯¹æ¯ä¸ªç½®ä¿¡åº¦é˜ˆå€¼è¿›è¡Œæµ‹è¯•
        all_results = {}
        
        for conf_thresh in conf_thresholds:
            print(f"\nğŸ“Š æµ‹è¯•ç½®ä¿¡åº¦é˜ˆå€¼: {conf_thresh}")
            print("-" * 40)
            
            threshold_results = []
            
            for i, image_path in enumerate(test_images):
                try:
                    print(f"   å¤„ç†å›¾ç‰‡ {i+1}/{len(test_images)}: {Path(image_path).name}")
                    analysis = self.analyze_single_image(image_path, conf_thresh)
                    threshold_results.append(analysis)
                except Exception as e:
                    print(f"   âš ï¸  åˆ†æå¤±è´¥: {Path(image_path).name} - {e}")
                    continue
            
            # è®¡ç®—æ€»ä½“ç»Ÿè®¡
            if threshold_results:
                overall_stats = self.calculate_overall_stats(threshold_results)
                
                # è®¡ç®— mAP
                map_results = self.calculate_map(threshold_results, iou_threshold=0.5)
                
                all_results[conf_thresh] = {
                    'individual_results': threshold_results,
                    'overall_stats': overall_stats,
                    'map_results': map_results
                }
                
                print(f"\nğŸ“ˆ ç½®ä¿¡åº¦ {conf_thresh} æ€»ä½“ç»Ÿè®¡:")
                print(f"   å¹³å‡ç²¾ç¡®åº¦: {overall_stats['avg_precision']:.3f}")
                print(f"   å¹³å‡å¬å›ç‡: {overall_stats['avg_recall']:.3f}")
                print(f"   å¹³å‡ F1 åˆ†æ•°: {overall_stats['avg_f1']:.3f}")
                print(f"   mAP@0.5: {map_results['mAP']:.3f}")
                print(f"   å¹³å‡ç½®ä¿¡åº¦å·®å¼‚: {overall_stats['avg_conf_diff']:.3f}")
                print(f"   å¹³å‡ IoU: {overall_stats['avg_iou']:.3f}")
                
                if len(test_images) >= 10:
                    print(f"   æµ‹è¯•å›¾ç‰‡æ•°é‡: {len(test_images)} (å……è¶³)")
                else:
                    print(f"   æµ‹è¯•å›¾ç‰‡æ•°é‡: {len(test_images)} (å»ºè®®å¢åŠ )")
        
        # ä¿å­˜ç»“æœ
        self.save_results(all_results)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self.generate_report(all_results)
        
        return all_results
    
    def calculate_overall_stats(self, results: List[Dict]) -> Dict:
        """è®¡ç®—æ€»ä½“ç»Ÿè®¡ä¿¡æ¯"""
        if not results:
            return {}
        
        precisions = [r['precision'] for r in results]
        recalls = [r['recall'] for r in results]
        f1_scores = [r['f1_score'] for r in results]
        conf_diffs = [r['avg_confidence_diff'] for r in results]
        ious = [r['avg_iou'] for r in results]
        
        return {
            'avg_precision': np.mean(precisions),
            'std_precision': np.std(precisions),
            'avg_recall': np.mean(recalls),
            'std_recall': np.std(recalls),
            'avg_f1': np.mean(f1_scores),
            'std_f1': np.std(f1_scores),
            'avg_conf_diff': np.mean(conf_diffs),
            'std_conf_diff': np.std(conf_diffs),
            'avg_iou': np.mean(ious),
            'std_iou': np.std(ious),
            'total_images': len(results)
        }
    
    def save_results(self, results: Dict):
        """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
        output_dir = Path("02_coreml_conversion") / "accuracy_results"
        output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = output_dir / f"accuracy_comparison_{int(time.time())}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_file}")
    
    def generate_report(self, results: Dict):
        """ç”Ÿæˆç²¾åº¦åˆ†ææŠ¥å‘Š"""
        if not results:
            return
        
        output_dir = Path("02_coreml_conversion") / "accuracy_results"
        output_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        report_file = output_dir / f"accuracy_report_{int(time.time())}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# YOLOv11 CoreML ç²¾åº¦å¯¹æ¯”æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"PyTorch æ¨¡å‹: {self.pytorch_model_path.name}\n")
            f.write(f"CoreML æ¨¡å‹: {self.coreml_model_path.name}\n\n")
            
            # æ€»ä½“ç»“è®º
            f.write("## ğŸ“Š æ€»ä½“ç»“è®º\n\n")
            
            best_f1_thresh = max(results.keys(), key=lambda k: results[k]['overall_stats']['avg_f1'])
            best_stats = results[best_f1_thresh]['overall_stats']
            best_map = results[best_f1_thresh]['map_results']
            
            f.write(f"- **æœ€ä½³ç½®ä¿¡åº¦é˜ˆå€¼**: {best_f1_thresh}\n")
            f.write(f"- **å¹³å‡ç²¾ç¡®åº¦**: {best_stats['avg_precision']:.3f} Â± {best_stats['std_precision']:.3f}\n")
            f.write(f"- **å¹³å‡å¬å›ç‡**: {best_stats['avg_recall']:.3f} Â± {best_stats['std_recall']:.3f}\n")
            f.write(f"- **å¹³å‡ F1 åˆ†æ•°**: {best_stats['avg_f1']:.3f} Â± {best_stats['std_f1']:.3f}\n")
            f.write(f"- **mAP@0.5**: {best_map['mAP']:.3f} (åŸºäº {best_map['valid_classes']}/{best_map['num_classes']} ä¸ªç±»åˆ«)\n")
            f.write(f"- **å¹³å‡ç½®ä¿¡åº¦å·®å¼‚**: {best_stats['avg_conf_diff']:.3f} Â± {best_stats['std_conf_diff']:.3f}\n")
            f.write(f"- **å¹³å‡ IoU**: {best_stats['avg_iou']:.3f} Â± {best_stats['std_iou']:.3f}\n")
            f.write(f"- **æµ‹è¯•å›¾ç‰‡æ•°é‡**: {best_stats['total_images']} å¼ \n\n")
            
            # ç²¾åº¦è¯„ä¼°
            if best_stats['avg_f1'] > 0.9:
                f.write("âœ… **ç²¾åº¦è¯„ä¼°**: ä¼˜ç§€ - CoreML è½¬æ¢åŸºæœ¬æ— ç²¾åº¦æŸå¤±\n\n")
            elif best_stats['avg_f1'] > 0.8:
                f.write("ğŸŸ¡ **ç²¾åº¦è¯„ä¼°**: è‰¯å¥½ - CoreML è½¬æ¢æœ‰è½»å¾®ç²¾åº¦æŸå¤±\n\n") 
            elif best_stats['avg_f1'] > 0.7:
                f.write("ğŸŸ  **ç²¾åº¦è¯„ä¼°**: ä¸€èˆ¬ - CoreML è½¬æ¢æœ‰æ˜æ˜¾ç²¾åº¦æŸå¤±\n\n")
            else:
                f.write("âŒ **ç²¾åº¦è¯„ä¼°**: è¾ƒå·® - CoreML è½¬æ¢ç²¾åº¦æŸå¤±ä¸¥é‡\n\n")
            
            # è¯¦ç»†ç»Ÿè®¡
            f.write("## ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡\n\n")
            f.write("| ç½®ä¿¡åº¦é˜ˆå€¼ | ç²¾ç¡®åº¦ | å¬å›ç‡ | F1åˆ†æ•° | ç½®ä¿¡åº¦å·®å¼‚ | å¹³å‡IoU |\n")
            f.write("|------------|---------|---------|---------|------------|----------|\n")
            
            for thresh, data in results.items():
                stats = data['overall_stats']
                f.write(f"| {thresh} | {stats['avg_precision']:.3f} | {stats['avg_recall']:.3f} | "
                       f"{stats['avg_f1']:.3f} | {stats['avg_conf_diff']:.3f} | {stats['avg_iou']:.3f} |\n")
            
            f.write("\n## ğŸ” åˆ†æå»ºè®®\n\n")
            
            if best_stats['avg_conf_diff'] > 0.1:
                f.write("- âš ï¸  ç½®ä¿¡åº¦å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹è½¬æ¢å‚æ•°\n")
            
            if best_stats['avg_iou'] < 0.8:
                f.write("- âš ï¸  IoU è¾ƒä½ï¼Œæ£€æµ‹æ¡†ä½ç½®å¯èƒ½æœ‰åå·®\n")
            
            if best_stats['avg_precision'] < 0.8:
                f.write("- âš ï¸  ç²¾ç¡®åº¦åä½ï¼Œå¯èƒ½å­˜åœ¨è¾ƒå¤šè¯¯æ£€\n")
            
            if best_stats['avg_recall'] < 0.8:
                f.write("- âš ï¸  å¬å›ç‡åä½ï¼Œå¯èƒ½é—æ¼è¾ƒå¤šå¯¹è±¡\n")
        
        print(f"ğŸ“„ ç²¾åº¦æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="YOLOv11 CoreML ç²¾åº¦å¯¹æ¯”å·¥å…·")
    parser.add_argument("--pytorch-model", required=True, help="PyTorch æ¨¡å‹è·¯å¾„")
    parser.add_argument("--coreml-model", required=True, help="CoreML æ¨¡å‹è·¯å¾„")
    parser.add_argument("--conf-thresholds", nargs="+", type=float, 
                       default=[0.1, 0.25, 0.5], help="ç½®ä¿¡åº¦é˜ˆå€¼åˆ—è¡¨")
    parser.add_argument("--dataset", type=str, default="auto",
                       choices=["auto", "coco_val_sample", "custom_sample", "yolo_test_images"],
                       help="æµ‹è¯•æ•°æ®é›†é€‰æ‹©")
    parser.add_argument("--download-datasets", action="store_true", 
                       help="é¢„å…ˆä¸‹è½½æ‰€æœ‰æµ‹è¯•æ•°æ®é›†")
    
    args = parser.parse_args()
    
    # å¦‚æœåªæ˜¯ä¸‹è½½æ•°æ®é›†
    if args.download_datasets:
        from dataset_manager import DatasetManager
        manager = DatasetManager()
        manager.download_all_datasets()
        return
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å­˜åœ¨æ€§
    if not Path(args.pytorch_model).exists():
        print(f"âŒ PyTorch æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.pytorch_model}")
        sys.exit(1)
    
    if not Path(args.coreml_model).exists():
        print(f"âŒ CoreML æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.coreml_model}")
        sys.exit(1)
    
    # è¿è¡Œç²¾åº¦å¯¹æ¯”
    comparator = AccuracyComparator(args.pytorch_model, args.coreml_model)
    results = comparator.run_full_comparison(args.conf_thresholds, args.dataset)
    
    if results:
        print("\nğŸ‰ ç²¾åº¦å¯¹æ¯”åˆ†æå®Œæˆï¼")
        print("\næŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š:")
        print(f"   - ç»“æœæ–‡ä»¶: 02_coreml_conversion/accuracy_results/")
        
        # æ˜¾ç¤ºæœ€ä½³ç»“æœæ‘˜è¦
        best_thresh = max(results.keys(), key=lambda k: results[k]['overall_stats']['avg_f1'])
        best_results = results[best_thresh]
        
        print(f"\nğŸ“Š æœ€ä½³ç»“æœæ‘˜è¦ (ç½®ä¿¡åº¦é˜ˆå€¼: {best_thresh}):")
        print(f"   F1 åˆ†æ•°: {best_results['overall_stats']['avg_f1']:.3f}")
        print(f"   mAP@0.5: {best_results['map_results']['mAP']:.3f}")
        print(f"   æµ‹è¯•å›¾ç‰‡: {best_results['overall_stats']['total_images']} å¼ ")
        
        if best_results['overall_stats']['avg_f1'] > 0.9:
            print("   ğŸ¯ ç²¾åº¦è¯„ä¼°: ä¼˜ç§€ - è½¬æ¢è´¨é‡å¾ˆé«˜")
        elif best_results['overall_stats']['avg_f1'] > 0.8:
            print("   ğŸŸ¡ ç²¾åº¦è¯„ä¼°: è‰¯å¥½ - è½¬æ¢è´¨é‡å¯æ¥å—")
        else:
            print("   âš ï¸  ç²¾åº¦è¯„ä¼°: éœ€è¦ä¼˜åŒ– - å»ºè®®æ£€æŸ¥è½¬æ¢å‚æ•°")
            
    else:
        print("âŒ ç²¾åº¦å¯¹æ¯”åˆ†æå¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()