#!/usr/bin/env python3
"""
æ•°æ®é›†ç®¡ç†å·¥å…·
ç”¨äºä¸‹è½½å’Œç®¡ç†ç²¾åº¦æµ‹è¯•æ•°æ®é›†
"""

import os
import sys
import json
import zipfile
import tarfile
from pathlib import Path
from typing import List, Dict, Optional
import requests
from tqdm import tqdm
import yaml

class DatasetManager:
    """æ•°æ®é›†ç®¡ç†å™¨"""
    
    def __init__(self, datasets_dir: str = "02_coreml_conversion/datasets"):
        """
        åˆå§‹åŒ–æ•°æ®é›†ç®¡ç†å™¨
        
        Args:
            datasets_dir: æ•°æ®é›†å­˜å‚¨ç›®å½•
        """
        self.datasets_dir = Path(datasets_dir)
        self.datasets_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®é›†é…ç½®
        self.available_datasets = {
            "coco_val_sample": {
                "name": "COCO éªŒè¯é›†æ ·æœ¬",
                "description": "COCO 2017 éªŒè¯é›†çš„ 100 å¼ å›¾ç‰‡æ ·æœ¬",
                "size": "~20MB",
                "images": 100,
                "url": None,  # æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªç²¾é€‰çš„æ ·æœ¬é›†
                "format": "coco"
            },
            "yolo_test_images": {
                "name": "YOLO å®˜æ–¹æµ‹è¯•å›¾ç‰‡",
                "description": "Ultralytics å®˜æ–¹æä¾›çš„æµ‹è¯•å›¾ç‰‡é›†",
                "size": "~5MB", 
                "images": 10,
                "urls": [
                    "https://ultralytics.com/images/bus.jpg",
                    "https://ultralytics.com/images/zidane.jpg",
                    "https://github.com/ultralytics/yolov5/raw/master/data/images/bus.jpg",
                    "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg",
                    "https://raw.githubusercontent.com/ultralytics/ultralytics/main/ultralytics/assets/bus.jpg",
                    "https://raw.githubusercontent.com/ultralytics/ultralytics/main/ultralytics/assets/zidane.jpg",
                ],
                "format": "images"
            },
            "custom_sample": {
                "name": "è‡ªå®šä¹‰æµ‹è¯•æ ·æœ¬",
                "description": "ç²¾é€‰çš„å¤šæ ·åŒ–æµ‹è¯•å›¾ç‰‡",
                "size": "~10MB",
                "images": 20,
                "format": "images"
            }
        }
    
    def download_file(self, url: str, filepath: Path, chunk_size: int = 8192) -> bool:
        """
        ä¸‹è½½æ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦æ¡
        
        Args:
            url: ä¸‹è½½é“¾æ¥
            filepath: ä¿å­˜è·¯å¾„
            chunk_size: å—å¤§å°
            
        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(filepath, 'wb') as f, tqdm(
                desc=filepath.name,
                total=total_size,
                unit='B',
                unit_scale=True
            ) as pbar:
                for chunk in response.iter_content(chunk_size=chunk_size):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))
            
            return True
            
        except Exception as e:
            print(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            if filepath.exists():
                filepath.unlink()
            return False
    
    def download_yolo_test_images(self) -> Path:
        """ä¸‹è½½ YOLO å®˜æ–¹æµ‹è¯•å›¾ç‰‡"""
        dataset_dir = self.datasets_dir / "yolo_test_images"
        dataset_dir.mkdir(exist_ok=True)
        
        print("ğŸ“¥ ä¸‹è½½ YOLO å®˜æ–¹æµ‹è¯•å›¾ç‰‡...")
        
        urls = self.available_datasets["yolo_test_images"]["urls"]
        successful_downloads = []
        
        for i, url in enumerate(urls):
            filename = f"test_image_{i+1:02d}.jpg"
            filepath = dataset_dir / filename
            
            if filepath.exists():
                print(f"   è·³è¿‡å·²å­˜åœ¨çš„æ–‡ä»¶: {filename}")
                successful_downloads.append(str(filepath))
                continue
            
            print(f"   ä¸‹è½½: {filename}")
            if self.download_file(url, filepath):
                successful_downloads.append(str(filepath))
            else:
                print(f"   âš ï¸  ä¸‹è½½å¤±è´¥: {filename}")
        
        # åˆ›å»ºæ•°æ®é›†ä¿¡æ¯æ–‡ä»¶
        dataset_info = {
            "name": "YOLO Test Images",
            "images": len(successful_downloads),
            "image_paths": successful_downloads,
            "format": "images",
            "created_at": str(Path().cwd())
        }
        
        info_file = dataset_dir / "dataset_info.json"
        with open(info_file, 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        print(f"âœ… æˆåŠŸä¸‹è½½ {len(successful_downloads)} å¼ å›¾ç‰‡åˆ° {dataset_dir}")
        return dataset_dir
    
    def create_custom_sample_dataset(self) -> Path:
        """åˆ›å»ºè‡ªå®šä¹‰æ ·æœ¬æ•°æ®é›†"""
        dataset_dir = self.datasets_dir / "custom_sample"
        dataset_dir.mkdir(exist_ok=True)
        
        print("ğŸ“¥ åˆ›å»ºè‡ªå®šä¹‰æµ‹è¯•æ ·æœ¬...")
        
        # å¤šæ ·åŒ–çš„æµ‹è¯•å›¾ç‰‡ URLs
        sample_urls = [
            # äº¤é€šåœºæ™¯
            "https://ultralytics.com/images/bus.jpg",
            "https://github.com/ultralytics/yolov5/raw/master/data/images/bus.jpg",
            
            # äººç‰©åœºæ™¯  
            "https://ultralytics.com/images/zidane.jpg",
            "https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg",
            
            # æ¥è‡ª COCO æ•°æ®é›†çš„ä»£è¡¨æ€§å›¾ç‰‡
            "https://farm4.staticflickr.com/3273/2602356079_2e0cc0b89e_z.jpg",  # çŒ«
            "https://farm6.staticflickr.com/5285/5221301158_b396b8b03b_z.jpg",  # ç‹—
            "https://farm5.staticflickr.com/4062/4695028316_4b2d1d4502_z.jpg",  # æ±½è½¦
            "https://farm3.staticflickr.com/2534/4181635046_36ab77a4d5_z.jpg",  # æ‘©æ‰˜è½¦
            "https://farm6.staticflickr.com/5241/5245510697_e8e7e85636_z.jpg",  # è‡ªè¡Œè½¦
            "https://farm5.staticflickr.com/4148/4997038311_9de2e8c3b4_z.jpg",  # é£æœº
        ]
        
        successful_downloads = []
        
        for i, url in enumerate(sample_urls):
            filename = f"sample_{i+1:02d}.jpg"
            filepath = dataset_dir / filename
            
            if filepath.exists():
                successful_downloads.append(str(filepath))
                continue
            
            print(f"   ä¸‹è½½æ ·æœ¬ {i+1}/{len(sample_urls)}: {filename}")
            if self.download_file(url, filepath):
                successful_downloads.append(str(filepath))
        
        # åˆ›å»ºæ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            "name": "Custom Sample Dataset",
            "description": "ç²¾é€‰çš„å¤šæ ·åŒ–æµ‹è¯•å›¾ç‰‡ï¼ŒåŒ…å«ä¸åŒåœºæ™¯å’Œå¯¹è±¡ç±»å‹",
            "images": len(successful_downloads),
            "image_paths": successful_downloads,
            "categories": ["person", "vehicle", "animal", "outdoor", "indoor"],
            "format": "images"
        }
        
        info_file = dataset_dir / "dataset_info.json"
        with open(info_file, 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        print(f"âœ… è‡ªå®šä¹‰æ ·æœ¬æ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(successful_downloads)} å¼ å›¾ç‰‡")
        return dataset_dir
    
    def download_coco_val_sample(self) -> Path:
        """ä¸‹è½½ COCO éªŒè¯é›†æ ·æœ¬"""
        dataset_dir = self.datasets_dir / "coco_val_sample"
        dataset_dir.mkdir(exist_ok=True)
        
        print("ğŸ“¥ å‡†å¤‡ COCO éªŒè¯é›†æ ·æœ¬...")
        
        # ç”±äºå®Œæ•´çš„ COCO æ•°æ®é›†å¾ˆå¤§ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç²¾é€‰æ ·æœ¬
        # åŒ…å«ä¸åŒå¤æ‚åº¦å’Œå¯¹è±¡æ•°é‡çš„å›¾ç‰‡
        coco_sample_urls = [
            # ç®€å•åœºæ™¯ï¼ˆ1-2ä¸ªå¯¹è±¡ï¼‰
            "http://images.cocodataset.org/val2017/000000000139.jpg",
            "http://images.cocodataset.org/val2017/000000000285.jpg",
            "http://images.cocodataset.org/val2017/000000000632.jpg",
            
            # ä¸­ç­‰å¤æ‚åº¦ï¼ˆ3-5ä¸ªå¯¹è±¡ï¼‰
            "http://images.cocodataset.org/val2017/000000000724.jpg", 
            "http://images.cocodataset.org/val2017/000000001268.jpg",
            "http://images.cocodataset.org/val2017/000000001503.jpg",
            
            # å¤æ‚åœºæ™¯ï¼ˆ6+ä¸ªå¯¹è±¡ï¼‰
            "http://images.cocodataset.org/val2017/000000002299.jpg",
            "http://images.cocodataset.org/val2017/000000002532.jpg",
            "http://images.cocodataset.org/val2017/000000002685.jpg",
            
            # ä¸åŒç±»åˆ«
            "http://images.cocodataset.org/val2017/000000003156.jpg",  # åŠ¨ç‰©
            "http://images.cocodataset.org/val2017/000000003501.jpg",  # é£Ÿç‰©
            "http://images.cocodataset.org/val2017/000000004395.jpg",  # è¿åŠ¨
            "http://images.cocodataset.org/val2017/000000005193.jpg",  # å®¤å†…
            "http://images.cocodataset.org/val2017/000000005529.jpg",  # äº¤é€š
            "http://images.cocodataset.org/val2017/000000006040.jpg",  # äººç¾¤
        ]
        
        print(f"ä¸‹è½½ {len(coco_sample_urls)} å¼  COCO æ ·æœ¬å›¾ç‰‡...")
        successful_downloads = []
        
        for i, url in enumerate(coco_sample_urls):
            # ä» URL ä¸­æå–å›¾ç‰‡ ID
            img_id = url.split('/')[-1]
            filepath = dataset_dir / img_id
            
            if filepath.exists():
                successful_downloads.append(str(filepath))
                continue
            
            print(f"   ä¸‹è½½ COCO å›¾ç‰‡ {i+1}/{len(coco_sample_urls)}: {img_id}")
            if self.download_file(url, filepath):
                successful_downloads.append(str(filepath))
        
        # åˆ›å»º COCO æ ¼å¼çš„æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            "name": "COCO Validation Sample",
            "description": "COCO 2017 éªŒè¯é›†çš„ç²¾é€‰æ ·æœ¬ï¼ŒåŒ…å«ä¸åŒå¤æ‚åº¦çš„åœºæ™¯",
            "images": len(successful_downloads),
            "image_paths": successful_downloads,
            "format": "coco",
            "categories": "COCO 80 classes",
            "complexity_levels": ["simple", "medium", "complex"],
            "source": "COCO 2017 validation set"
        }
        
        info_file = dataset_dir / "dataset_info.json"
        with open(info_file, 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        print(f"âœ… COCO æ ·æœ¬æ•°æ®é›†å‡†å¤‡å®Œæˆ: {len(successful_downloads)} å¼ å›¾ç‰‡")
        return dataset_dir
    
    def list_available_datasets(self):
        """åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†"""
        print("ğŸ“‹ å¯ç”¨çš„æµ‹è¯•æ•°æ®é›†:")
        print("=" * 60)
        
        for key, info in self.available_datasets.items():
            print(f"\nğŸ”¸ {key}")
            print(f"   åç§°: {info['name']}")
            print(f"   æè¿°: {info['description']}")
            print(f"   å¤§å°: {info['size']}")
            print(f"   å›¾ç‰‡æ•°é‡: {info['images']}")
            
            # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
            dataset_path = self.datasets_dir / key
            if dataset_path.exists():
                info_file = dataset_path / "dataset_info.json"
                if info_file.exists():
                    with open(info_file, 'r') as f:
                        local_info = json.load(f)
                    print(f"   çŠ¶æ€: âœ… å·²ä¸‹è½½ ({local_info['images']} å¼ å›¾ç‰‡)")
                else:
                    print(f"   çŠ¶æ€: âš ï¸  ç›®å½•å­˜åœ¨ä½†ä¿¡æ¯ä¸å®Œæ•´")
            else:
                print(f"   çŠ¶æ€: âŒ æœªä¸‹è½½")
    
    def download_dataset(self, dataset_name: str) -> Optional[Path]:
        """
        ä¸‹è½½æŒ‡å®šçš„æ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            æ•°æ®é›†è·¯å¾„ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
        """
        if dataset_name not in self.available_datasets:
            print(f"âŒ æœªçŸ¥çš„æ•°æ®é›†: {dataset_name}")
            return None
        
        print(f"å¼€å§‹ä¸‹è½½æ•°æ®é›†: {dataset_name}")
        
        if dataset_name == "yolo_test_images":
            return self.download_yolo_test_images()
        elif dataset_name == "custom_sample":
            return self.create_custom_sample_dataset()
        elif dataset_name == "coco_val_sample":
            return self.download_coco_val_sample()
        else:
            print(f"âŒ æ•°æ®é›† {dataset_name} çš„ä¸‹è½½æ–¹æ³•æœªå®ç°")
            return None
    
    def get_dataset_info(self, dataset_name: str) -> Optional[Dict]:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        dataset_path = self.datasets_dir / dataset_name
        info_file = dataset_path / "dataset_info.json"
        
        if not info_file.exists():
            return None
        
        with open(info_file, 'r') as f:
            return json.load(f)
    
    def get_dataset_images(self, dataset_name: str) -> List[str]:
        """è·å–æ•°æ®é›†ä¸­çš„æ‰€æœ‰å›¾ç‰‡è·¯å¾„"""
        info = self.get_dataset_info(dataset_name)
        if info is None:
            return []
        
        return info.get('image_paths', [])
    
    def download_all_datasets(self):
        """ä¸‹è½½æ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
        print("ğŸš€ ä¸‹è½½æ‰€æœ‰æµ‹è¯•æ•°æ®é›†...")
        
        for dataset_name in self.available_datasets.keys():
            print(f"\n{'='*60}")
            result = self.download_dataset(dataset_name)
            if result:
                print(f"âœ… {dataset_name} ä¸‹è½½å®Œæˆ")
            else:
                print(f"âŒ {dataset_name} ä¸‹è½½å¤±è´¥")
        
        print(f"\n{'='*60}")
        print("ğŸ‰ æ‰€æœ‰æ•°æ®é›†ä¸‹è½½å®Œæˆ!")
        self.list_available_datasets()

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ•°æ®é›†ç®¡ç†å·¥å…·")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºå¯ç”¨æ•°æ®é›†")
    parser.add_argument("--download", type=str, help="ä¸‹è½½æŒ‡å®šæ•°æ®é›†")
    parser.add_argument("--download-all", action="store_true", help="ä¸‹è½½æ‰€æœ‰æ•°æ®é›†")
    parser.add_argument("--info", type=str, help="æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯")
    
    args = parser.parse_args()
    
    manager = DatasetManager()
    
    if args.list:
        manager.list_available_datasets()
    
    elif args.download:
        result = manager.download_dataset(args.download)
        if result:
            print(f"\nâœ… æ•°æ®é›†å·²ä¿å­˜åˆ°: {result}")
        else:
            print(f"\nâŒ æ•°æ®é›†ä¸‹è½½å¤±è´¥")
    
    elif args.download_all:
        manager.download_all_datasets()
    
    elif args.info:
        info = manager.get_dataset_info(args.info)
        if info:
            print(f"\nğŸ“‹ æ•°æ®é›†ä¿¡æ¯: {args.info}")
            print("=" * 40)
            for key, value in info.items():
                print(f"{key}: {value}")
        else:
            print(f"âŒ æœªæ‰¾åˆ°æ•°æ®é›†ä¿¡æ¯: {args.info}")
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()